{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86b1dc04-af50-4aa4-854b-d3d96cf28260",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "#%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-chroma bs4\n",
    "#!pip install langchain-cohere\n",
    "#!pip install -U langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b8ff99-9f71-400e-a8ed-fb882484a37d",
   "metadata": {},
   "source": [
    "# Setup & Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad1a564-307f-4416-ad2c-f0ca7afec844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the secret keys from the JSON file\n",
    "def load_secret_keys(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        secret_keys = json.load(file)\n",
    "    return secret_keys\n",
    "keys =load_secret_keys('secrets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0307ba-5e70-400c-ae1a-bcd9715c1f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = keys['Groq']\n",
    "os.environ[\"COHERE_API_KEY\"] = keys['cohere']\n",
    "os.environ[\"GROQ_API_KEY\"] = keys['Groq']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbafa56-322f-4468-8e2e-35706a8e275e",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe67cc93-fcd3-43c9-a104-e4152feb0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c57c1e-a059-4753-9b9d-ac127dda53bb",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae88ff7-66ae-47d0-bcfc-6a8782932867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd2aaa9a-8939-4af6-bdc7-e0891dba7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "url = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "#url = 'https://jalammar.github.io/illustrated-transformer/'\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(url,),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=CohereEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f768a647-c337-4009-ac55-d88b50de39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use ten sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24682048-4411-45c3-b426-e6caca0cbf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down a complex task into smaller, simpler, and more manageable steps. This technique is used to enhance model performance on complex tasks, allowing the model to \"think step by step\" and decompose hard tasks into smaller and simpler steps.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Forbidden\"}\\')')\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Task Decomposition?\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "018831ce-8e7f-49cb-a69b-b75b3f0c65f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reflexion is a framework that equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. It is designed to help agents learn from their mistakes and adapt to new situations by allowing them to reflect on their past actions and decisions.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"What is Reflexion?\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfc428-289b-4991-aaa6-5e21cf6266f6",
   "metadata": {},
   "source": [
    "## Adding chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09ad7914-a1c9-4cec-b5f1-a0f08e41cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6154e8e-0287-4df5-90b4-ccf8ce9a1127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complex task into smaller and simpler steps. This is done to make the task more manageable and to enable an agent to plan and execute the task more effectively.\n",
      "I don't have enough context to provide a specific answer. Could you please provide more information or clarify what \"it\" refers to? I'll do my best to assist you with common ways of doing whatever \"it\" is.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_1[\"answer\"])\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5567282c-3b4a-4d52-bd27-024a38e96500",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d5dd3ad-9946-4b9b-bdce-c15837de78b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down a complex task into smaller and simpler steps. This technique is used to decompose hard tasks into manageable tasks, which can be solved more easily.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "095fe1b0-47b2-4aa8-b1c3-7712ebcda832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m not sure what \"it\" refers to. Could you please clarify what you\\'re asking about? If you provide more context or information, I\\'ll do my best to help you.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc3eee5f-3cc1-47a3-a23b-1dac39a4398b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Task Decomposition?\n",
      "\n",
      "AI: Task decomposition is the process of breaking down a complex task into smaller and simpler steps. This technique is used to decompose hard tasks into manageable tasks, which can be solved more easily.\n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: I'm not sure what \"it\" refers to. Could you please clarify what you're asking about? If you provide more context or information, I'll do my best to help you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb5d42-abeb-4cc7-9375-994623c97426",
   "metadata": {},
   "source": [
    "## Tying it together\n",
    "\n",
    "![Conversational Retrieval Chain](https://python.langchain.com/v0.2/assets/images/conversational_retrieval_chain-5c7a96abe29e582bc575a0a0d63f86b0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94e5b393-1cbe-4241-8963-41fdfb484d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "#from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\",temperature=0)\n",
    "\n",
    "\n",
    "### Construct retriever ###\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=CohereEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4007547-8cd1-46e3-9d49-dfdc3b28d08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique used to break down complex tasks into smaller, simpler steps. This is done by instructing a model to \"think step by step\" and utilize more test-time computation to decompose hard tasks into manageable tasks.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f6b181c-e9a2-4be0-a732-49654c48b535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Common ways of doing Task Decomposition include using simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", or task-specific instructions, such as \"Write a story outline.\" for writing a novel, or even human inputs.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a93206-e8d9-413d-9639-2b66d7c06d08",
   "metadata": {},
   "source": [
    "# Agents\n",
    "\n",
    "Agents leverage the reasoning capabilities of LLMs to make decisions during execution. Using agents allow you to offload some discretion over the retrieval process. Although their behavior is less predictable than chains, they offer some advantages in this context:\n",
    "\n",
    "- Agents generate the input to the retriever directly, without necessarily needing us to explicitly build in contextualization, as we did above;\n",
    "- Agents can execute multiple retrieval steps in service of a query, or refrain from executing a retrieval step altogether (e.g., in response to a generic greeting from a user)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86a3df5-1f72-494b-8c99-172b3b602f80",
   "metadata": {},
   "source": [
    "## Retrieval tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9cd58788-122e-4864-a29e-1bd7a9ac4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4fbec8b-3233-4b8b-8a16-018802ac159e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.invoke(\"task decomposition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82bd3eb-cdd7-48ca-9f86-377e874b0fec",
   "metadata": {},
   "source": [
    "## Agent constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca6b72bd-0051-4cb5-a82f-5d193ae9ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "112ada2e-35a9-4a5c-aa0f-7082cb558b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_n18w', 'function': {'arguments': '{\"query\":\"What is task decomposition?\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 951, 'total_tokens': 1023, 'completion_time': 0.057928367, 'prompt_time': 0.154055392, 'queue_time': None, 'total_time': 0.21198375900000002}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-29c98e64-c9e5-4d21-84d2-6714c4eab611-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'What is task decomposition?'}, 'id': 'call_n18w'}], usage_metadata={'input_tokens': 951, 'output_tokens': 72, 'total_tokens': 1023})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', name='blog_post_retriever', tool_call_id='call_n18w')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_6kw2', 'function': {'arguments': '{\"query\":\"What is task decomposition?\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 1510, 'total_tokens': 1553, 'completion_time': 0.034268491, 'prompt_time': 0.225484054, 'queue_time': None, 'total_time': 0.259752545}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8dc96930-d35d-47b4-a17e-4293a0ed69bc-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'What is task decomposition?'}, 'id': 'call_6kw2'}], usage_metadata={'input_tokens': 1510, 'output_tokens': 43, 'total_tokens': 1553})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', name='blog_post_retriever', tool_call_id='call_6kw2')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='You\\'ve already called the tool for tool call id \"call_6kw2\" and it yielded the same result. Since you\\'ve already received the information, I\\'ll provide a direct response.\\n\\nTask decomposition is a process that breaks down a complex task into smaller, more manageable subtasks. This can be done using various techniques, such as Chain of Thought (CoT) or Tree of Thoughts. CoT involves instructing a model to think step by step, while Tree of Thoughts explores multiple reasoning possibilities at each step. Task decomposition can be achieved through simple prompting, task-specific instructions, or human inputs.', response_metadata={'token_usage': {'completion_tokens': 123, 'prompt_tokens': 2096, 'total_tokens': 2219, 'completion_time': 0.096860803, 'prompt_time': 0.377429275, 'queue_time': None, 'total_time': 0.474290078}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_873a560973', 'finish_reason': 'stop', 'logprobs': None}, id='run-8fd7cc9a-cde2-4aa1-8ca5-16b04a429bf8-0', usage_metadata={'input_tokens': 2096, 'output_tokens': 123, 'total_tokens': 2219})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Task Decomposition?\"\n",
    "try:\n",
    "    # Attempt to stream the messages from the agent_executor\n",
    "    for s in agent_executor.stream({\"messages\": [HumanMessage(content=query)]}):\n",
    "        print(s)    # Print each message from the stream\n",
    "        print(\"----\")  # Print separator for clarity\n",
    "except Exception as e:  # Handle any exception that occurs\n",
    "    print(\"*\"*25)\n",
    "    print(e)  # Print the exception message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "978f7a65-4787-4449-b108-9a09000562e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87783430-c4ae-44db-9b87-3f9dc6da144a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='Hi Bob!', response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 946, 'total_tokens': 950, 'completion_time': 0.002406277, 'prompt_time': 0.141447366, 'queue_time': None, 'total_time': 0.143853643}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_af05557ca2', 'finish_reason': 'stop', 'logprobs': None}, id='run-0754fade-d0a0-4548-acd3-9112c90dd8a4-0', usage_metadata={'input_tokens': 946, 'output_tokens': 4, 'total_tokens': 950})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "for s in agent_executor.stream({\"messages\": [HumanMessage(content=\"Hi! I'm bob\")]}, config=config):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ab8c28c-cc64-4409-9242-3360030468af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_0cac', 'function': {'arguments': '{\"query\":\"common ways of doing it\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 80, 'prompt_tokens': 954, 'total_tokens': 1034, 'completion_time': 0.062800286, 'prompt_time': 0.217456711, 'queue_time': None, 'total_time': 0.280256997}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-3744f83c-055f-423e-b123-403523e2c4e6-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'common ways of doing it'}, 'id': 'call_0cac'}], usage_metadata={'input_tokens': 954, 'output_tokens': 80, 'total_tokens': 1034})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', name='blog_post_retriever', tool_call_id='call_0cac')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_cr18', 'function': {'arguments': '{\"query\":\"common ways of doing it redo the search\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 1421, 'total_tokens': 1464, 'completion_time': 0.033531349, 'prompt_time': 0.28203347, 'queue_time': None, 'total_time': 0.315564819}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_af05557ca2', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-dfa25996-abda-44e9-b92b-fea722ebb97b-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'common ways of doing it redo the search'}, 'id': 'call_cr18'}], usage_metadata={'input_tokens': 1421, 'output_tokens': 43, 'total_tokens': 1464})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='GOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"', name='blog_post_retriever', tool_call_id='call_cr18')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='I think I can respond directly without using a tool. It seems that the previous searches didn\\'t yield any specific information about common ways of doing it. Instead, it seems that the system is providing general guidelines and constraints for the LLM to operate within. \\n\\nIf you\\'re looking for common ways of doing something, I\\'d be happy to help you brainstorm or provide some general information on the topic. Could you please provide more context or clarify what you mean by \"common ways of doing it\"?', response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 2162, 'total_tokens': 2262, 'completion_time': 0.078798532, 'prompt_time': 0.351109436, 'queue_time': None, 'total_time': 0.429907968}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-60891409-886e-4907-a638-1fc7b1682320-0', usage_metadata={'input_tokens': 2162, 'output_tokens': 100, 'total_tokens': 2262})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What according to the blog post are common ways of doing it? redo the search\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a2a7f3b-070d-4463-ae16-932060225a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_adgw', 'function': {'arguments': '{\"query\":\"task decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 2049, 'total_tokens': 2086, 'completion_time': 0.028903229, 'prompt_time': 0.352043083, 'queue_time': None, 'total_time': 0.38094631199999995}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_873a560973', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-87133322-51f8-4c52-9d7a-2cb50ae6a976-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'task decomposition'}, 'id': 'call_adgw'}], usage_metadata={'input_tokens': 2049, 'output_tokens': 37, 'total_tokens': 2086})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', name='blog_post_retriever', tool_call_id='call_adgw')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='It seems like the tool call id \"call_adgw\" yielded a result about Task Decomposition. According to the text, Task Decomposition is a process that breaks down complex tasks into smaller and simpler steps. This can be done using various techniques such as Chain of Thought (CoT), Tree of Thoughts, or simple prompting. The goal of Task Decomposition is to make complex tasks more manageable and to provide a clearer understanding of the thinking process involved.', response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 2651, 'total_tokens': 2743, 'completion_time': 0.074427316, 'prompt_time': 0.412840151, 'queue_time': None, 'total_time': 0.487267467}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-485d1f1f-02b7-4c63-95ac-1e66ce24bfc7-0', usage_metadata={'input_tokens': 2651, 'output_tokens': 92, 'total_tokens': 2743})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Task Decomposition?\"\n",
    "try:\n",
    "    for s in agent_executor.stream({\"messages\": [HumanMessage(content=query)]}, config=config):\n",
    "        print(s)\n",
    "        print(\"----\")\n",
    "except Exception as e:  # Handle any exception that occurs\n",
    "    print(\"*\"*25)\n",
    "    print(e)  # Print the exception message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d953b5a4-e581-4d70-9f11-b4119305fcef",
   "metadata": {},
   "source": [
    "# Complete code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3eeaffac-00f1-40f6-b466-1493aac71747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "#from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import os\n",
    "import json\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\",temperature=0)\n",
    "\n",
    "\n",
    "### Construct retriever ###\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=CohereEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "### Build retriever tool ###\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
    ")\n",
    "tools = [tool]\n",
    "\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae6d09ea-6f07-48a9-bded-03ce3beb5f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_2rek', 'function': {'arguments': '{\"query\":\"common ways of doing it\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 936, 'total_tokens': 1009, 'completion_time': 0.057310101, 'prompt_time': 0.140738811, 'queue_time': None, 'total_time': 0.198048912}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_873a560973', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-3b6d27e6-7ebc-4b16-a171-c47d707a61f3-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'common ways of doing it'}, 'id': 'call_2rek'}], usage_metadata={'input_tokens': 936, 'output_tokens': 73, 'total_tokens': 1009})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.', name='blog_post_retriever', tool_call_id='call_2rek')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_wztn', 'function': {'arguments': '{\"query\":\"common ways of doing it redo the search\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 1413, 'total_tokens': 1456, 'completion_time': 0.033905979, 'prompt_time': 0.314600573, 'queue_time': None, 'total_time': 0.348506552}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-eba72a21-f89a-4041-bcd6-43a0b253cfbc-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'common ways of doing it redo the search'}, 'id': 'call_wztn'}], usage_metadata={'input_tokens': 1413, 'output_tokens': 43, 'total_tokens': 1456})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='GOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes', name='blog_post_retriever', tool_call_id='call_wztn')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='It seems that the tool call id \"call_wztn\" did not yield any relevant information. The output appears to be a set of constraints and goals, but it does not provide any information about common ways of doing it.', response_metadata={'token_usage': {'completion_tokens': 47, 'prompt_tokens': 1968, 'total_tokens': 2015, 'completion_time': 0.037654826, 'prompt_time': 0.296536448, 'queue_time': None, 'total_time': 0.33419127400000004}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-97712bd2-b8ff-4939-8be3-1cecf0e176af-0', usage_metadata={'input_tokens': 1968, 'output_tokens': 47, 'total_tokens': 2015})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"What according to the blog post are common ways of doing it? redo the search\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8c00b66-f716-4ced-8810-38c3f6777173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'LangGraph',\n",
       " 'nodes': {'__start__': {'name': None,\n",
       "   'bound': {'name': None, 'input_type': None, 'func': None, 'afunc': None},\n",
       "   'kwargs': {},\n",
       "   'config': {'tags': ['langsmith:hidden']},\n",
       "   'config_factories': [],\n",
       "   'custom_input_type': None,\n",
       "   'custom_output_type': None,\n",
       "   'channels': ['__start__'],\n",
       "   'triggers': ['__start__'],\n",
       "   'mapper': None,\n",
       "   'writers': [ChannelWrite<messages>(recurse=True, writes=[ChannelWriteEntry(channel='messages', value=<object object at 0x75be5c9dbc60>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages']),\n",
       "    ChannelWrite<start:agent>(recurse=True, writes=[ChannelWriteEntry(channel='start:agent', value='__start__', skip_none=False, mapper=None)], require_at_least_one_of=None)]},\n",
       "  'agent': {'name': None,\n",
       "   'bound': RunnableLambda(call_model),\n",
       "   'kwargs': {},\n",
       "   'config': {'tags': []},\n",
       "   'config_factories': [],\n",
       "   'custom_input_type': None,\n",
       "   'custom_output_type': None,\n",
       "   'channels': {'messages': 'messages',\n",
       "    'is_last_step': langgraph.managed.is_last_step.IsLastStepManager},\n",
       "   'triggers': ['tools', 'start:agent'],\n",
       "   'mapper': functools.partial(<function _coerce_state at 0x75be56d1df80>, <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>),\n",
       "   'writers': [ChannelWrite<agent,messages>(recurse=True, writes=[ChannelWriteEntry(channel='agent', value='agent', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x75be5c9dbc60>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages']),\n",
       "    _route(recurse=True, _is_channel_writer=True)]},\n",
       "  'tools': {'name': None,\n",
       "   'bound': tools(recurse=True, tools_by_name={'blog_post_retriever': Tool(name='blog_post_retriever', description='Searches and returns excerpts from the Autonomous Agents blog post.', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x75beb1258b80>, retriever=VectorStoreRetriever(tags=['Chroma', 'CohereEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x75be5c576330>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x75beb1258fe0>, retriever=VectorStoreRetriever(tags=['Chroma', 'CohereEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x75be5c576330>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))}),\n",
       "   'kwargs': {},\n",
       "   'config': {'tags': []},\n",
       "   'config_factories': [],\n",
       "   'custom_input_type': None,\n",
       "   'custom_output_type': None,\n",
       "   'channels': {'messages': 'messages',\n",
       "    'is_last_step': langgraph.managed.is_last_step.IsLastStepManager},\n",
       "   'triggers': ['branch:agent:should_continue:tools'],\n",
       "   'mapper': functools.partial(<function _coerce_state at 0x75be56d1df80>, <class 'langgraph.prebuilt.chat_agent_executor.AgentState'>),\n",
       "   'writers': [ChannelWrite<tools,messages>(recurse=True, writes=[ChannelWriteEntry(channel='tools', value='tools', skip_none=False, mapper=None), ChannelWriteEntry(channel='messages', value=<object object at 0x75be5c9dbc60>, skip_none=False, mapper=_get_state_key(recurse=False))], require_at_least_one_of=['messages'])]}},\n",
       " 'channels': {'messages': <langgraph.channels.binop.BinaryOperatorAggregate at 0x75be5ceb0470>,\n",
       "  '__start__': <langgraph.channels.ephemeral_value.EphemeralValue at 0x75be5ceb34d0>,\n",
       "  'agent': <langgraph.channels.ephemeral_value.EphemeralValue at 0x75be5cea46b0>,\n",
       "  'tools': <langgraph.channels.ephemeral_value.EphemeralValue at 0x75be5c617170>,\n",
       "  'start:agent': <langgraph.channels.ephemeral_value.EphemeralValue at 0x75be5c6172f0>,\n",
       "  'branch:agent:should_continue:tools': <langgraph.channels.ephemeral_value.EphemeralValue at 0x75be5c615a00>},\n",
       " 'auto_validate': False,\n",
       " 'stream_mode': 'updates',\n",
       " 'output_channels': ['messages'],\n",
       " 'stream_channels': ['messages'],\n",
       " 'interrupt_after_nodes': [],\n",
       " 'interrupt_before_nodes': [],\n",
       " 'input_channels': '__start__',\n",
       " 'step_timeout': None,\n",
       " 'debug': False,\n",
       " 'checkpointer': <langgraph.checkpoint.sqlite.SqliteSaver at 0x75be56d12510>,\n",
       " 'retry_policy': None,\n",
       " 'config_type': None,\n",
       " 'builder': <langgraph.graph.state.StateGraph at 0x75be5cec1d60>}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

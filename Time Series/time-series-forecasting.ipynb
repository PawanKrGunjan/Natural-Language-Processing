{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install yfinance","metadata":{"id":"dvlZLKTrbbgc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import defaultdict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nsns.set(style='whitegrid', palette = 'muted', font_scale=1.2)\n\nColour_Palette = ['#01BEFE', '#FF7D00', '#FFDD00', '#FF006D', '#ADFF02', '#8F00FF']\n\nsns.set_palette(sns.color_palette(Colour_Palette))\n\ntqdm.pandas()","metadata":{"id":"BgQY5ZMMdCLz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Load Data","metadata":{"id":"am06BFNk2n2U"}},{"cell_type":"code","source":"import yfinance as yf\nfrom datetime import date, timedelta, datetime\n\nend_date =  date.today().strftime(\"%Y-%m-%d\")\nstart_date = '1990-01-01'\n\ndf = yf.download('AAPL', start=start_date, end=end_date)","metadata":{"id":"1CsQdNARdCYo","outputId":"f93cc8ce-c997-478b-9f48-6176b4469c0a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"id":"FGHNcP7LFDve","outputId":"9b796e7d-c5e4-43b7-b4db-c6d27615f6bd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"id":"1fkfFLexPqUD","outputId":"bf386b50-97e0-4fc3-a810-7bb902bee0e9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.dates as mdates # Formatting dates\n\ndef data_plot(df):\n  # Plot line charts\n  df_plot = df.copy()\n\n  ncols = 2\n  nrows = int(round(df_plot.shape[1] / ncols, 0))\n\n  fig, ax = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, figsize=(14, 7))\n  for i, ax in enumerate(fig.axes):\n          sns.lineplot(data = df_plot.iloc[:, i], ax=ax)\n          ax.tick_params(axis=\"x\", rotation=30, labelsize=10, length=0)\n          ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n  fig.tight_layout()\n  plt.show()","metadata":{"id":"L9oeChNUFzri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_plot(df)","metadata":{"id":"ou8JXdwEF2gP","outputId":"61e2905f-d209-447b-c1f1-a36fc623b941"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pre processing","metadata":{"id":"m3Ug0olUFhYZ"}},{"cell_type":"code","source":"#Train test Split\nimport math\n# Setting 80 percent data for training\ntraining_data_len = math.ceil(len(df) * .8)\ntraining_data_len\n\n#Splitting the dataset\ntrain_data = df[:training_data_len].iloc[:,:1]\ntest_data = df[training_data_len:].iloc[:,:1]\nprint(train_data.shape, test_data.shape)","metadata":{"id":"_MkQzojMNgjw","outputId":"d6082575-92ff-486b-cc2c-fc8e7b8baf20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting Open Price values\ndataset_train = train_data.Open.values\n# Reshaping 1D to 2D array\ndataset_train = np.reshape(dataset_train, (-1,1))\ndataset_train.shape","metadata":{"id":"fK0Uvu08P3Wy","outputId":"23d5a859-a0d4-4c48-992b-0ba02c0dc393"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))\n# scaling dataset\nscaled_train = scaler.fit_transform(dataset_train)\n\nprint(scaled_train[:5])","metadata":{"id":"O8GKUx8hP7-f","outputId":"20302a3a-f1c3-4bb6-b632-c04a61e719a1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_length = 50","metadata":{"id":"Nl5ALjXOYvb4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Selecting Open Price values\ndataset_test = test_data.Open.values\n# Reshaping 1D to 2D array\ndataset_test = np.reshape(dataset_test, (-1,1))\n# Normalizing values between 0 and 1\nscaled_test = scaler.fit_transform(dataset_test)\nprint(*scaled_test[:5])","metadata":{"id":"cylDYQR8QHtT","outputId":"99ecc95a-5afd-4451-9e80-f8188b0e584a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create sequences and labels for training data\nsequence_length = 30  # Number of time steps to look back\nX_train, y_train = [], []\nfor i in range(len(scaled_train) - sequence_length):\n    X_train.append(scaled_train[i:i+sequence_length])\n    y_train.append(scaled_train[i+1:i+sequence_length+1])\nX_train, y_train = np.array(X_train), np.array(y_train)\n\n# Convert data to PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32)\nX_train.shape,y_train.shape","metadata":{"id":"RibQkFrbPqUF","outputId":"7feea457-2cce-4966-86e3-5465033e8de2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create sequences and labels for training data\nsequence_length = 30  # Number of time steps to look back\nX_test, y_test = [], []\nfor i in range(len(scaled_test) - sequence_length):\n    X_test.append(scaled_test[i:i+sequence_length])\n    y_test.append(scaled_test[i+1:i+sequence_length+1])\nX_test, y_test = np.array(X_test), np.array(y_test)\n\n# Convert data to PyTorch tensors\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32)\nX_test.shape, y_test.shape","metadata":{"id":"nkZnfre9PqUF","outputId":"4a037f6a-2e25-4629-f228-2def583f161f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"id":"ghM6M2PrXY4I","outputId":"c63be3ad-bd86-4a32-a067-d6bb546870f3"},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"X_train[:2]","metadata":{"id":"4mx3OaHLXe6v","outputId":"c4963de0-1b31-418e-bc4c-af008573d869"}},{"cell_type":"code","source":"y_train.shape","metadata":{"id":"n52hvc44XkkC","outputId":"940134c4-b106-4b09-c716-fc24ddc6fc1f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train[0]","metadata":{"id":"7UqTE4h3Xulr","outputId":"b88c3325-6c1c-4016-b1b4-dd092019d621"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"LSTM Model","metadata":{"id":"VKN1TnmRADza"}},{"cell_type":"code","source":"# Define an LSTM-based model\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers):\n        super(LSTMModel, self).__init__()\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.linear = nn.Linear(hidden_size, 1)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        out = self.linear(out)\n        return out","metadata":{"id":"nHmtMbodPqUG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(y_train.shape)","metadata":{"id":"3RstXOOrPqUG","outputId":"6d00c3ce-fc09-4111-8ae9-7c7acb31b612"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n# Create DataLoader for batch training\ninput_size = 1\nnum_layers = 2\nhidden_size = 32\noutput_size = 1\n\n# Define the model, loss function, and optimizer\nmodel = LSTMModel(input_size, hidden_size, num_layers).to(device)\n\nloss_fn = torch.nn.MSELoss(reduction='mean')\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nprint(model)","metadata":{"id":"XcgQflU-PqUG","outputId":"74a33577-09d4-42cf-8d59-94d00dcc4253"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\n# Create DataLoader for batch training\ntrain_dataset = torch.utils.data.TensorDataset(X_train, y_train)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Create DataLoader for batch training\ntest_dataset = torch.utils.data.TensorDataset(X_test, y_test)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"ITgnDyUFPqUG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(next(iter(train_loader))[0].shape)\nprint(next(iter(train_loader))[1].shape)","metadata":{"id":"Kj2niCPdPqUG","outputId":"bc0507f6-588b-48f8-ce5e-cc61444ec737"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 50\ntrain_hist =[]\ntest_hist =[]\n# Training loop\nfor epoch in range(num_epochs):\n    total_loss = 0.0\n\n    # Training\n    model.train()\n    for batch_X, batch_y in train_loader:\n        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n        predictions = model(batch_X)\n        loss = loss_fn(predictions, batch_y)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    # Calculate average training loss and accuracy\n    average_loss = total_loss / len(train_loader)\n    train_hist.append(average_loss)\n\n    # Validation on test data\n    model.eval()\n    with torch.no_grad():\n        total_test_loss = 0.0\n\n        for batch_X_test, batch_y_test in test_loader:\n            batch_X_test, batch_y_test = batch_X_test.to(device), batch_y_test.to(device)\n            predictions_test = model(batch_X_test)\n            test_loss = loss_fn(predictions_test, batch_y_test)\n\n            total_test_loss += test_loss.item()\n\n        # Calculate average test loss and accuracy\n        average_test_loss = total_test_loss / len(test_loader)\n        test_hist.append(average_test_loss)\n    if (epoch+1)%10==0:\n        print(f'Epoch [{epoch+1}/{num_epochs}] - Training Loss: {average_loss:.4f}, Test Loss: {average_test_loss:.4f}')","metadata":{"id":"6fy0KimPPqUG","outputId":"32ac933c-4d51-4cf6-cf68-822c880872be"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.linspace(1,num_epochs,num_epochs)\nplt.plot(x,train_hist,scalex=True, label=\"Training loss\")\nplt.plot(x, test_hist, label=\"Test loss\")\nplt.legend()\nplt.show()","metadata":{"id":"v5Tue5PKazDq","outputId":"18464ed0-fcde-4f91-aa63-8f1380bde3c7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting Opening Price","metadata":{"id":"xtEeWBtDZJFs"}},{"cell_type":"code","source":"# Define the number of future time steps to forecast\nnum_forecast_steps = 30\n\n# Select a specific sequence from X_test to plot (for example, the first sequence)\n# Convert to NumPy and remove singleton dimensions\nsequence_to_plot = X_test.squeeze().cpu().numpy()\n\n# Initialize a sequence of historical data (e.g., the last part of your original time series)\n# Make sure this sequence has the same format and scaling as your training data\n# Use the last 30 data points as the starting point\nhistorical_data = sequence_to_plot[-1]\nprint(historical_data.shape)\n\n# Initialize a list to store the forecasted values\nforecasted_values = []\n\n# Use the trained model to forecast future values\nwith torch.no_grad():\n    for _ in range(num_forecast_steps*2):\n        # Prepare the historical_data tensor\n        historical_data_tensor = torch.as_tensor(historical_data).view(1, -1, 1).float().to(device)\n        # Use the model to predict the next value\n        predicted_value = model(historical_data_tensor).cpu().numpy()[0, 0]\n\n        # Append the predicted value to the forecasted_values list\n        forecasted_values.append(predicted_value[0])\n\n        # Update the historical_data sequence by removing the oldest value and adding the predicted value\n        historical_data = np.roll(historical_data, shift=-1)\n        historical_data[-1] = predicted_value","metadata":{"id":"BIOVRSTpRX-O","outputId":"d44fbb57-9073-4894-e3b0-a73cb7fe4fdb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate futute dates\nlast_date = test_data.index[-1]\n\n# Generate the next 30 dates\nfuture_dates = pd.date_range(start=last_date + pd.DateOffset(1), periods=30)\n\n# Concatenate the original index with the future dates\ncombined_index = test_data.index.append(future_dates)\n","metadata":{"id":"pZfyzcvgol5s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(test_data.index[-100:-30], test_data.Open[-100:-30], label = \"train_data\", color = \"b\")\noriginal_cases = scaler.inverse_transform(np.expand_dims(sequence_to_plot[-1], axis=0)).flatten()\n\nplt.plot(test_data.index[-30:], original_cases, label='Original Data', color='green')\n\nforecasted_cases = scaler.inverse_transform(np.expand_dims(forecasted_values, axis=0)).flatten()\nplt.plot(combined_index[-60:], forecasted_cases, label='Forecasted Values', color='red')\n\n\nplt.xlabel('Time Step')\nplt.ylabel('Value')\nplt.legend()\nplt.title('Time Series Forecasting')\nplt.grid(True)\nplt.show()","metadata":{"id":"QJBGgWRvoopM","outputId":"4e8689d8-637e-4952-e707-d0e128e93d79"},"execution_count":null,"outputs":[]}]}